{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Welcome! In this notebook we will see how to use sklearn to perform classification\n",
      "# first let's import the necessary libraries:\n",
      "import numpy as np\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.svm import LinearSVC, SVC\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "# sklearn is divided in several sub-libraries for the different functionality, for\n",
      "# a complete overview see the API here: http://scikit-learn.org/stable/modules/classes.html"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Next let's load some data to play with. In this notebook we will use\n",
      "# the Abalone dataset from the UCI repository. The dataset should be put \n",
      "# in the same folder as the notebook. \n",
      "\n",
      "# with this dictionary we will encode the categorical feature 'gender'.\n",
      "cat_gender = {'I':np.array([1,0,0]), 'M':np.array([0,1,0]), 'F':np.array([0,0,1])}\n",
      "# loading the data and massaging it to become usable \n",
      "abalone_data = [x.strip().split(',') for x in open('abalone/abalone.data').readlines()] \n",
      "# Construct an array with the categorical feature (first one) and the remaining features\n",
      "abalone_data = np.array([np.hstack((cat_gender[x[0]],np.array(map(float,x[1:])))) for x in abalone_data])\n",
      "# Take last column as label\n",
      "abalone_labels = abalone_data[:,-1].astype('int')\n",
      "# The remaining columns are the data\n",
      "abalone_data = abalone_data[:,:-1]\n",
      "# We split the data randomly between train and test\n",
      "naba= abalone_labels.shape[0]\n",
      "sel = range(naba)\n",
      "np.random.seed(25)\n",
      "np.random.shuffle(sel)\n",
      "aba_train_data = abalone_data[sel[:naba/2],:]\n",
      "aba_train_labs = abalone_labels[sel[:naba/2]]\n",
      "aba_test_data = abalone_data[sel[naba/2:],:]\n",
      "aba_test_labs = abalone_labels[sel[naba/2:]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# K-NN\n",
      "# We define a function to run the K-NN classifier with a range of 'k' values\n",
      "def runknn(tr_data, tr_labels, te_data, te_labels):\n",
      "    for k in range(1,15,2):\n",
      "        # Here we use the KNN function from scikit learn (in the\n",
      "        # homework you will have to implement it yourselves) \n",
      "        # First we create a classifier object\n",
      "        knn = KNeighborsClassifier(k)\n",
      "        # Then we \"train\" it with the training data\n",
      "        knn.fit(aba_train_data, aba_train_labs)\n",
      "        # Finally we compute the accuracy of the test data\n",
      "        acc = knn.score(aba_test_data, aba_test_labs)\n",
      "        print 'Accuracy (k=%d): %.4f'%(k,acc)\n",
      "# We call the function with the train and test splits\n",
      "runknn(aba_train_data, aba_train_labs, aba_test_data, aba_test_labs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Logistic Regression\n",
      "# Similarly as in KNN, we first create a classifier object, with C=1\n",
      "LogReg = LogisticRegression(C=1)\n",
      "# Then we train it with the training data\n",
      "LogReg.fit(aba_train_data, aba_train_labs)\n",
      "# And finally use the score function to get the accuracy\n",
      "print 'Log Reg accuracy', LogReg.score(aba_test_data, aba_test_labs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# But, is C=1 the best regularization parameter? We will\n",
      "# find out with cross-validation + grid search\n",
      "# sklearn has a handy function for that too:\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "# we define the search space for the C parameter\n",
      "parameters = {'C':[10**i for i in range(-7,7)]}\n",
      "# and create a grid search object with the type of classifier we want to\n",
      "# use and the parameters\n",
      "clf = GridSearchCV(LogisticRegression(), parameters, verbose=False)\n",
      "# and we train the classifier with grid search (this will use cross-validation\n",
      "# to select the best parameter)\n",
      "clf.fit(aba_train_data, aba_train_labs)\n",
      "print 'Log Reg with Grid search', clf.score(aba_test_data, aba_test_labs), 'with C =', clf.best_params_['C']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Linear Support Vector Machine\n",
      "# The interface for the different classifiers is very similar; the Linear SVM\n",
      "# works in the same way as the logistic regression\n",
      "clf = LinearSVC(C=1)\n",
      "clf.fit(aba_train_data, aba_train_labs)\n",
      "print 'Linear SVM score', clf.score(aba_test_data, aba_test_labs)\n",
      "parameters = {'C':[10**i for i in range(-3, 4)]}\n",
      "clf = GridSearchCV(LinearSVC(), parameters, verbose=False)\n",
      "clf.fit(aba_train_data, aba_train_labs)\n",
      "print 'Linear SVM score with Grid Search', clf.score(aba_test_data, aba_test_labs), 'with C =', clf.best_params_['C']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now we will do the same with the kernelized SVM\n",
      "clf = SVC(C=1)\n",
      "clf.fit(aba_train_data, aba_train_labs)\n",
      "print 'RBF SVM score', clf.score(aba_test_data, aba_test_labs)\n",
      "# We can actually define several parameters we want to optimize over:\n",
      "parameters = {'C':[10**i for i in range(-3, 4)], 'kernel':['linear', 'rbf']}\n",
      "clf = GridSearchCV(SVC(), parameters, verbose=False)\n",
      "clf.fit(aba_train_data, aba_train_labs)\n",
      "print 'RBF SVM score with Grid Search', clf.score(aba_test_data, aba_test_labs), 'with C =', \\\n",
      "    parameters.best_params_['C'], 'and kernel =', parameters.best_params_['kernel']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}